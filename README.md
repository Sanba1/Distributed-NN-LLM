# Distributed Neural Network for On-Device LLM Inference  
*A mobile-first distributed AI system built using MediaPipe and quantized LLM models.*

## ðŸ“– Overview
This project demonstrates a distributed neural network architecture where multiple mobile devices collaboratively perform LLM inference. Each phone runs a small, quantized language model locally using **Google MediaPipe**, processes a subset of the data, and sends partial results back to a central server for merging.

The system reduces response time and server load by using **data-parallel inference** across devices.

This work was developed as part of my Bachelor's thesis.

---

## ðŸš€ Features
- Runs a **quantized LLM model locally** on Android using MediaPipe
- Distributed processing across multiple phones
- Local vector search over document chunks
- Lightweight JSON-based communication
- Central summarizer merges partial results into a final answer
- Works fully offline except for sending/receiving messages
- Low memory footprint (mobile-friendly architecture)

---

## ðŸ“‚ Project Structure
app/
â””â”€â”€ src/
â””â”€â”€ main/
â”œâ”€â”€ java/
â”‚   â””â”€â”€ com/google/mediapipe/examples/llminference/
â”‚        â”œâ”€â”€ MainActivity.kt
â”‚        â”œâ”€â”€ InferenceModel.kt
â”‚        â”œâ”€â”€ ChunkRepository.kt
â”‚        â”œâ”€â”€ QueryData.kt
â”‚        â”œâ”€â”€ FirebaseListener.kt
â”‚        â”œâ”€â”€ LoadJsonFile.kt
â”‚        â”œâ”€â”€ ChatScreen.kt
â”‚        â”œâ”€â”€ ChatViewModel.kt
â”‚        â”œâ”€â”€ QueryListenerService.kt
â”‚        â”œâ”€â”€ SelectionScreen.kt
â”‚        â”œâ”€â”€ LoginActivity.kt
â”‚        â”œâ”€â”€ (and other UI + logic files)
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€llm/ # Place only quantized int4.bin model 
â”‚   â”‚    â””â”€â”€ model.task
â”‚   â”œâ”€â”€ json/ # Place PDF chunks with embeddings
â”‚   â”‚    â”œâ”€â”€ chunk_1.json
â”‚   â”‚    â”œâ”€â”€ chunk_2.json
â”‚   â”‚    â””â”€â”€ ...
â”‚   â””â”€â”€ config/               # Optional configs
â”‚
â”œâ”€â”€ AndroidManifest.xml
â””â”€â”€ res/

### ðŸ“Œ Key Components

**`assets/llm/`**  
Contains the quantized MediaPipe LLM model (`model.task`).  
This is the only model required for on-device inference.

**`assets/json/`**  
Contains the preprocessed PDF chunks with embeddings.  
Each device performs local vector search over these files.

**`java/.../llminference/`**  
This is where the main logic lives:
- loading the LLM model  
- running inference  
- reading JSON chunks  
- computing cosine similarity  
- sending partial results to the server (Firestore)  

**`res/layout/`**  
UI layout for the Android app.

---
